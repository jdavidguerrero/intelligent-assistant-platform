# Week 2 Summary â€” RAG Question-Answering System

**Status**: âœ… Complete (Days 1-5)
**Total Tests**: 354 (all passing except 3 golden set edge cases)
**Knowledge Base**: 9,670 chunks (18 PDFs + 754 Pete Tong Academy md + 320 YouTube transcripts)

---

## Overview

Week 2 transformed the retrieval system from Week 1 into a **full RAG assistant** that answers music production questions with citation-backed responses. The system can now:

1. Accept natural language questions via `POST /ask`
2. Retrieve relevant context from the knowledge base
3. Generate grounded answers using OpenAI or Anthropic LLMs
4. Cite sources with inline references `[1]`, `[2]`, `[3]`
5. Refuse to answer when confidence is low or topic is out-of-domain

---

## Day 1 â€” PDF Ingestion + Page Metadata

### Implementation
- **PDF loader** using `pdfplumber` with ligature normalization
- **Page-aware chunking**: Added `page_number: int | None` to `Chunk` dataclass and `ChunkRecord` ORM model
- **Backward-compatible schema**: Nullable `page_number` field supports both PDFs (with pages) and md/txt (without pages)

### Files Changed
- `core/chunking.py` â€” Added `page_number` field to `Chunk`
- `core/text.py` â€” `extract_pdf_text()` with ligature fixes
- `core/types.py` â€” Added `page_number` to `ChunkDict`
- `db/models.py` â€” Added `page_number: Mapped[int | None]`
- `ingestion/loaders.py` â€” `load_pdf_pages()` using pdfplumber
- `ingestion/ingest.py` â€” Page-aware chunking for PDFs

### Results
- **Ingested**: 18 PDFs â†’ 8,916 chunks
- **Tests**: 230 â†’ 294 (all passing)

---

## Day 2 â€” Generation Protocol + Context Assembly + Prompts

### Implementation
- **GenerationProvider protocol**: Runtime-checkable protocol following same pattern as `EmbeddingProvider`
- **Factory pattern**: `create_generation_provider()` switches between OpenAI/Anthropic via `LLM_PROVIDER` env var
- **Context assembly**: `format_context_block()` creates numbered citation blocks, `format_source_list()` deduplicates sources
- **Prompt templates**: System prompt with grounding constraint ("ONLY from context"), citation rules `[1]`, refusal behavior

### Files Changed
- `core/generation/base.py` â€” `Message`, `GenerationRequest`, `GenerationResponse`, `GenerationProvider` protocol
- `ingestion/generation.py` â€” `OpenAIGenerationProvider`, `AnthropicGenerationProvider`, factory
- `api/deps.py` â€” `get_generation_provider()` singleton
- `core/rag/context.py` â€” `RetrievedChunk`, `format_context_block()`, `format_source_list()`
- `core/rag/prompts.py` â€” `SYSTEM_PROMPT`, `build_system_prompt()`, `build_user_prompt()`
- `.env.example` â€” Added `LLM_PROVIDER=openai`, `ANTHROPIC_API_KEY`

### Results
- **Tests**: 294 â†’ 325 (all passing)
- **New dependency**: `anthropic==0.79.0`

---

## Day 3 â€” Citation Parser + Confidence Threshold + `/ask` Endpoint

### Implementation
- **Citation parser**: Regex `\[(\d+)\]` extraction + range validation
- **Confidence threshold**: Reject requests when `max_score < 0.7` with HTTPException 422 `insufficient_knowledge`
- **`/ask` endpoint**: 9-step pipeline:
  1. Query expansion + embedding (reuse from `/search`)
  2. Vector search + reranking (reuse from `/search`)
  3. `evaluate_confidence(scores)` â†’ refuse if too low
  4. `format_context_block(chunks)` + `format_source_list(chunks)`
  5. `build_system_prompt()` + `build_user_prompt()`
  6. `generator.generate(request)` â†’ LLM answer
  7. `validate_citations(answer, source_map)` â†’ parsed citations
  8. Build `AskResponse` with timing metadata

### Files Changed
- `core/rag/citations.py` â€” `Citation`, `CitationResult`, `extract_citation_indices()`, `validate_citations()`
- `core/rag/confidence.py` â€” `ConfidenceDecision`, `evaluate_confidence()`
- `api/schemas/ask.py` â€” `AskRequest`, `SourceReference`, `UsageMetadata`, `AskResponse`
- `api/routes/ask.py` â€” `POST /ask` with 9-step pipeline
- `api/main.py` â€” Include ask router
- `tests/conftest.py` â€” `FakeGenerationProvider`, `ask_client` fixture

### Results
- **Tests**: 325 (all passing)
- **Live endpoint**: Successfully answered "How should I EQ kick for house music?" with citations

---

## Day 4-5 â€” Golden Set + Reranking Improvements

### Implementation
- **Course boost increase**: 1.15 â†’ 1.25 (25% boost for Pete Tong courses) to balance PDF dominance
- **Golden set**: 15 music production Q&A pairs + 10 hallucination queries + 3 disambiguation queries
- **Parametrized tests**: pytest `@pytest.mark.parametrize` for automated validation
- **Manual validation test**: Citation accuracy + hallucination refusal rate measurement

### Files Changed
- `api/routes/search.py` â€” `course_boost=1.25`
- `api/routes/ask.py` â€” `course_boost=1.25`
- `db/rerank.py` â€” Default `course_boost=1.25`, updated docstrings
- `tests/test_golden_set.py` â€” NEW: comprehensive golden set with 29 tests

### Golden Set Results
- **Musical queries**: 13/15 passed (87%) with `confidence_threshold=0.58`
- **Hallucination refusal**: 10/10 (100%) â€” perfect out-of-domain detection
- **Disambiguation**: 2/3 passed (67%)
- **Known gaps**:
  - "Prepare stems for DJ set" (score 0.54 â€” insufficient coverage)
  - Some genre-specific queries need lower threshold (0.5-0.55)

### YouTube Content Validation
- **320 chunks** (3.3% of corpus) from transcribed videos
- **Genres covered**: organic house, melodic house, chord progressions, psychoacoustics
- **Test query**: "How to make organic house like Anjunadeep?" â†’ Top 5 sources ALL YouTube videos
- **Integration**: YouTube sources rank highly when relevant (e.g., rank #2-3 for melodic house queries)

---

## Architecture Highlights

### Layer Separation (Maintained)
```
api/ â†’ ingestion/ â†’ core/     âœ… Clean
api/ â†’ db/                     âœ… Clean
ingestion/ â†’ db/               âœ… Clean
ingestion/ â†’ core/             âœ… Clean

core/ â†’ db/                    âŒ NEVER
core/ â†’ api/                   âŒ NEVER
core/ â†’ ingestion/             âŒ NEVER
```

### Key Patterns
1. **Protocols over inheritance**: `GenerationProvider`, `EmbeddingProvider` use structural typing
2. **Factory pattern**: `create_generation_provider()`, `create_embedding_provider()` for env-var switching
3. **Frozen dataclasses**: Immutable value objects (`Chunk`, `Message`, `Citation`, etc.)
4. **Explicit error handling**: `HTTPException 422` for insufficient_knowledge, warnings for invalid_citations
5. **Observability**: Request IDs, timing metadata, response headers for distributed tracing

---

## Test Coverage

| Module | Tests | Coverage Notes |
|--------|-------|---------------|
| `core/chunking.py` | 25 | Token-based chunking, overlaps, page numbers |
| `core/text.py` | 39 | PDF ligatures, whitespace normalization |
| `core/rag/citations.py` | 21 | Regex extraction, range validation |
| `core/rag/context.py` | 17 | Context formatting, source deduplication |
| `core/rag/prompts.py` | 16 | System prompt, user prompt assembly |
| `api/routes/ask.py` | 10 | Happy path, low confidence, invalid citations |
| `api/routes/search.py` | 41 | Reranking, MMR, query expansion |
| `tests/test_golden_set.py` | 29 | End-to-end RAG validation |
| **Total** | **354** | **All passing (except 3 edge cases)** |

---

## Performance Metrics

### Retrieval + Generation Latency
- **Embedding**: ~50-100ms (cached), ~300ms (uncached)
- **Search**: ~50-150ms (pgvector HNSW)
- **Reranking**: ~5-10ms
- **Generation**: ~2-5s (OpenAI gpt-4o), ~3-7s (Anthropic claude-sonnet-4)
- **Total**: ~3-7s end-to-end

### Knowledge Base Stats
- **Total chunks**: 9,670
- **PDFs**: 8,916 (92.2%)
- **Pete Tong Academy**: 754 (7.8%)
- **YouTube transcripts**: 320 (3.3%)
- **Embedding dimension**: 1536 (text-embedding-3-small)
- **Index**: HNSW with `m=16`, `ef_construction=64`

---

## Known Issues & Future Work

### Known Gaps
1. **"Prepare stems for DJ set"** â€” Score 0.54, insufficient coverage in corpus
2. **Genre-specific queries** â€” Some need lower confidence threshold (0.5-0.55)
3. **Disambiguation edge cases** â€” "house compressor" doesn't find all expected topics

### Future Improvements
1. **Query rewriting**: Use LLM to rephrase ambiguous queries before retrieval
2. **Hybrid search**: Combine dense (pgvector) + sparse (BM25) retrieval
3. **Multi-hop reasoning**: Break complex questions into sub-questions
4. **Feedback loop**: User ratings â†’ fine-tune embeddings + reranking weights
5. **Streaming responses**: SSE for real-time answer generation
6. **Source snippets**: Highlight exact text spans used for each citation

---

## Commands

### Run all tests
```bash
pytest -q
```

### Run golden set
```bash
pytest tests/test_golden_set.py -v --tb=short
```

### Run manual validation
```bash
pytest tests/test_golden_set.py::test_citation_accuracy_manual --runxfail -s
```

### Start API server
```bash
uvicorn api.main:app --reload
```

### Test ask endpoint
```bash
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{"query": "How should I mix kick and bass together in house music?"}'
```

---

## Week 2 Deliverables âœ…

- [x] Day 1: PDF ingestion + page metadata
- [x] Day 2: Generation protocol + context assembly + prompts
- [x] Day 3: Citation parser + confidence threshold + `/ask` endpoint
- [x] Day 4: Golden set + hallucination tests + disambiguation
- [x] Day 5: Citation accuracy measurement + reranking improvements + documentation

**Week 2 Complete!** ðŸŽ‰
